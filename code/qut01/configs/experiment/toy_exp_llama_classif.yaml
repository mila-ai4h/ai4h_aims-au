# @package _global_

# To execute this experiment, run one of the entrypoints while specifying this config, e.g.:
#     python train.py experiment=distilbert_classif.yaml

defaults:
  - /model/optimization: default.yaml
  - /model/metrics: multi_label_metrics.yaml
  # - /model/metrics: binary_classifier_metrics.yaml # use this for binary classifier
  - override /model: binary_cross_entropy.yaml
  - override /data: sentence_sampler.yaml
  - override /callbacks: classification.yaml
  - override /logger:
      # - comet_online.yaml
      - tensorboard.yaml
  - _self_

experiment_name: toy_exp_classif # experiment name used in output paths
backbone_model_name: NousResearch/Llama-2-7b-hf
target_label_key: relevance # tensor used for model training/evaluation purposes
target_metric: valid/f1-macro # name of the metric we will be targeting for hparam optimization
target_metric_mode: max # optimization mode of the target metric defined above

utils:
  print_config: True
  project_name: ${utils.framework_name}-frozen # name for all experiments with frozen backbones
  default_num_workers: 0

model: # specifies which LightningModule-derived implementation provides the training/inference logic
  _recursive_: False
  _target_: qut01.models.classif.transformer.TransformerClassifier
  pretrained_model:
    _target_: transformers.LlamaForSequenceClassification.from_pretrained
    pretrained_model_name_or_path: ${backbone_model_name}
    num_labels: ${data.num_criteria}
  num_output_classes: ${data.num_criteria}
  output_tensor_extra_dims:
    - ${data.num_criteria}
  head: null
  pretrained_model_fine_tuning_mode:
    type: frozen
    except:
      - score
  model_output_param_name: logits
  model_config_dim_attrib_name: hidden_size
  label_key: ${target_label_key}
  sample_count_to_render: 10 # approximate/optimistic upper bound; increase if needed!
  ignore_index: ${data.label_ignore_index} # only applicable when using hard labels
  log_train_batch_size_each_step: true
  batch_count_hints: ${data.batch_count_hints} # need to specify hints due to iterable data loader

data:
  #classif_setup: a-s-c1
  #num_criteria: 3
  tokenizer:
    _target_: transformers.AutoTokenizer.from_pretrained
    pretrained_model_name_or_path: ${backbone_model_name}
  datamodule:
    split_ratios: # sending most of the data on train - it will be limited by "limit_train_batches"
      train: 1.0
  sentence_batch_size: 2 # number of sentences loaded per data loader iteration
  #sentence_collate_fn:  # useful to add prompt (see last input)
  #  _partial_: False
  #  _target_: qut01.data.transforms.collate.StatementSentenceCollater
  #  tokenizer: ${data.tokenizer}
  #  apply_ignore_index: ${data.label_ignore_index}
  #  keys_to_batch_manually: null
  #  prompt_text_to_prepend: "not sure"

trainer:
  # precision: 16-mixed
  max_epochs: 3
  log_every_n_steps: 1
  num_sanity_val_steps: -1
  limit_train_batches: 4
  accelerator: cpu # llora does not support mps on mac
  devices: auto

profiler: # in case we run the data profiler, these are the modified setting to use
  use_parser: False
  batch_count: 100
  loop_count: 10
