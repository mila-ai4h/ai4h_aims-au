# @package _global_

# To execute this experiment, run one of the entrypoints while specifying this config, e.g.:
#     python train.py experiment=distilbert_classif.yaml

defaults:
  - /model/optimization: default.yaml
  - /model/metrics: multi_label_metrics.yaml
  #- /model/metrics: binary_classifier_metrics.yaml # use this for binary classifier
  - override /model: binary_cross_entropy.yaml
  - override /data: sentence_sampler.yaml
  - override /callbacks: classification.yaml
  - override /logger:
      - comet_online.yaml
      - tensorboard.yaml
  - _self_

experiment_name: frozen_distilbert_classif # experiment name used in output paths
backbone_model_name: distilbert-base-uncased
target_label_key: relevance # tensor used for model training/evaluation purposes
target_metric: valid/f1-macro # name of the metric we will be targeting for hparam optimization
target_metric_mode: max # optimization mode of the target metric defined above

utils:
  print_config: True
  project_name: ${utils.framework_name}-frozen # name for all experiments with frozen backbones
  default_num_workers: 4

model: # specifies which LightningModule-derived implementation provides the training/inference logic
  _recursive_: False
  _target_: qut01.models.classif.transformer.TransformerClassifier
  pretrained_model:
    #_target_: transformers.AutoModelForSequenceClassification.from_pretrained
    #_target_: transformers.PretrainedModel.from_pretrained
    _target_: transformers.DistilBertModel.from_pretrained
    pretrained_model_name_or_path: ${backbone_model_name}
  head:
    _target_: qut01.models.components.simple_mlp.SimpleMLP
    hidden_channels: [500] # arbitrary
    out_channels: ${data.num_criteria}
    with_batch_norm: false
    dropout: 0.1
  num_output_classes: ${data.num_criteria}
  output_tensor_extra_dims:
    - ${data.num_criteria}
  pretrained_model_fine_tuning_mode:
    type: frozen
    except: []
  model_output_param_name: last_hidden_state
  model_config_dim_attrib_name: dim
  label_key: ${target_label_key}
  sample_count_to_render: 20 # approximate/optimistic upper bound; increase if needed!
  ignore_index: ${data.label_ignore_index} # only applicable when using hard labels
  log_train_batch_size_each_step: true
  batch_count_hints: ${data.batch_count_hints} # need to specify hints due to iterable data loader

data:
  classif_setup: any
  num_criteria: 11
  tokenizer:
    _target_: transformers.AutoTokenizer.from_pretrained
    pretrained_model_name_or_path: ${backbone_model_name}
  sentence_batch_size: 512 # number of sentences loaded per data loader iteration

trainer:
  precision: 16-mixed
  max_epochs: 100
  log_every_n_steps: 1
  num_sanity_val_steps: -1

profiler: # in case we run the data profiler, these are the modified setting to use
  use_parser: False
  batch_count: 100
  loop_count: 10
