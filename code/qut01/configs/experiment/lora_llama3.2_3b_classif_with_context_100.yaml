# @package _global_

# To execute this experiment, run one of the entrypoints while specifying this config, e.g.:
#     python train.py experiment=distilbert_classif.yaml

defaults:
  - /model/optimization: default.yaml
  - /model/metrics: multi_label_metrics.yaml
  # - /model/metrics: binary_classifier_metrics.yaml # use this for binary classifier
  - override /model: binary_cross_entropy.yaml
  - override /data: sentence_sampler.yaml
  - override /callbacks: classification.yaml
  - override /logger:
      - comet_online.yaml
      - tensorboard.yaml
  - _self_

#run_name: fixed
experiment_name: lora_llama3.2_3b_CONTEXT_100 # experiment name used in output paths
backbone_model_name: meta-llama/Llama-3.2-3B
target_label_key: relevance # tensor used for model training/evaluation purposes
target_metric: valid/f1-macro # name of the metric we will be targeting for hparam optimization
target_metric_mode: max # optimization mode of the target metric defined above

utils:
  print_config: True
  project_name: ${utils.framework_name}-lora # name for all experiments with frozen backbones
  default_num_workers: 6

wrapped_tokenizer:
  _target_: qut01.models.classif.llama3_utils.wrap_llama3_tokenizer
  wrapped_tokenizer:
    _target_: transformers.AutoTokenizer.from_pretrained
    pretrained_model_name_or_path: ${backbone_model_name}
    model_max_length: 350

model: # specifies which LightningModule-derived implementation provides the training/inference logic
  _recursive_: False
  _target_: qut01.models.classif.transformer.TransformerClassifier
  pretrained_model:
    _target_: qut01.models.classif.llama3_utils.wrap_llama3_model
    wrapped_tokenizer: ${wrapped_tokenizer}
    pretrained_model_name_or_path: ${backbone_model_name}
    num_labels: ${data.num_criteria}
  num_output_classes: ${data.num_criteria}
  output_tensor_extra_dims:
    - ${data.num_criteria}
  head: null
  pretrained_model_fine_tuning_mode:
    type: lora
  model_output_param_name: logits
  model_config_dim_attrib_name: hidden_size
  label_key: ${target_label_key}
  sample_count_to_render: 20 # approximate/optimistic upper bound; increase if needed!
  ignore_index: ${data.label_ignore_index} # only applicable when using hard labels
  log_train_batch_size_each_step: true
  batch_count_hints: ${data.batch_count_hints} # need to specify hints due to iterable data loader
  optimization:
    optimizer:
      _target_: torch.optim.Adam
      lr: 0.00003

data:
  classif_setup: any
  num_criteria: 11
  tokenizer: ${wrapped_tokenizer}
  sentence_batch_size: 8 # number of sentences loaded per data loader iteration
  context_word_count: 100
  left_context_boundary_token: "<|start_header_id|>"
  right_context_boundary_token: "<|end_header_id|>"
  #sentence_collate_fn:  # useful to add prompt (see last input)
  #  _partial_: False
  #  _target_: qut01.data.transforms.collate.StatementSentenceCollater
  #  tokenizer: ${data.tokenizer}
  #  apply_ignore_index: ${data.label_ignore_index}
  #  keys_to_batch_manually: null
  #  prompt_text_to_prepend: "not sure"

trainer:
  precision: 16-mixed
  max_epochs: 100
  log_every_n_steps: 1
  num_sanity_val_steps: -1
  accumulate_grad_batches: 8
  limit_train_batches: 2000

profiler: # in case we run the data profiler, these are the modified setting to use
  use_parser: False
  batch_count: 100
  loop_count: 10
