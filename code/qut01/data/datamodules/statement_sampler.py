"""Implements a data module for the QUT01-AIMS annotated statements dataset.

Note: the code below assumes that the dataset has been prepared with the following scripts:

    qut01/data/scripts/parse_raw_statements_data.py  (to prepare the raw PDF data)
    qut01/data/scripts/add_annotations_to_raw_dataset.py  (to add annotations)

The result should be a deeplake dataset located in the "data" directory of the project, i.e.
the directory you should have configured as an environment variable during framework setup. You
may also directly download the dataset from the QUT01-AIMS google drive shared folder. Unzip the
deeplake dataset in the "data" directory of the project, and it should be ready to be used by
this datamodule.
"""
import copy
import pathlib
import typing

import deeplake
import hydra.utils
import torch

import qut01.data.classif_utils
import qut01.data.split_utils
import qut01.data.statement_utils
import qut01.data.transforms
import qut01.utils.logging
from qut01.data.datamodules.base import BaseDataModule
from qut01.data.dataset_parser import DataParser, dataset_validated_branch_name

if typing.TYPE_CHECKING:
    import torch.utils.data

    from qut01.data import ClassifSetupType, SubsetType, VariableSubsetType

logger = qut01.utils.logging.get_logger(__name__)


class DataModule(BaseDataModule):
    """Datamodule for the QUT01-AIMS annotated dataset.

    This implementation will create data loaders that iterate over statements to create processed
    sentence data with classification labels based on annotations (and, optionally, randomly picked
    sentence to act as extra negatives) to provide examples for the criterion-wise classification
    of text as "relevant" or "irrelevant", and as "positive" or "negative" evidence.

    The `prepare_data` method of this datamodule will make sure that the dataset is available and
    ready to be used, and prepare the data split based on known statement entities/trademarks. The
    `setup` method will create the actual dataset parsers and dataloaders (should be called on the
    device where the data will ultimately be used).

    Args:
        data_path: parent directory where the dataset (.deeplake) is located.
        classif_setup: specifies the classification setup to use in order to generate target
            tensors. If `any`, then all statements with any kind of annotation will be loaded.
            Otherwise, only statements with annotations matching the provided annotation name
            (or meta group name) will be loaded.
        split_ratios: dictionary with ratios for each of the dataloader that will be prepared.
            All the values provided must sum to one.
        split_seed: random seed to use when preparing the training and validation data. This
            may be changed to generate more train-valid folds.
        expected_criteria_count: the number of criteria that are expected to have
            predictions generated by the model; this parameter is only used for validation purposes
            (i.e. to make sure configs stay consistent between model and data), and depends
            directly on the value provided for the `classif_setup` parameter.
        dataparser_configs: configuration dictionary of data parser settings, separated by
            data subset type, which may contain for example definitions for data transforms.
        dataloader_configs: configuration dictionary of data loader settings, separated by data
            subset type, which may contain for example batch sizes and worker counts.
        save_hyperparams: toggles whether hyperparameters should be saved in this class. This
            should be `False` when this class is derived, and the `save_hyperparameters` function
            should be called in the derived constructor.
    """

    def __init__(
        self,
        data_path: typing.Union[typing.AnyStr, pathlib.Path],
        classif_setup: "ClassifSetupType",
        split_ratios: typing.Dict["VariableSubsetType", float],  # for train/valid only!
        split_seed: int,  # for train/valid only!
        use_gold_set: bool,
        expected_criteria_count: int,  # used for validation purposes only
        dataparser_configs: typing.Optional[qut01.utils.DictConfig] = None,
        dataloader_configs: typing.Optional[qut01.utils.DictConfig] = None,
        save_hyperparams: bool = True,  # turn this off in derived classes
    ):
        """Initializes the data module.

        Note: it might look like we're not using the provided args at all, but we are actually
        automatically saving those to the `hparams` attribute (via the `save_hyperparameters`
        function) in order to use them later.
        """
        if save_hyperparams:
            # this line allows us to access hparams with `self.hparams` + auto-stores them in checkpoints
            self.save_hyperparameters(logger=False)  # logger=False since we don't need duplicated logs
        super().__init__(dataparser_configs=dataparser_configs, dataloader_configs=dataloader_configs)
        assert data_path is not None, "missing dataset path"
        data_path = pathlib.Path(data_path)
        assert data_path.exists(), f"invalid data path: {data_path}"
        self.data_path = data_path
        assert (
            classif_setup is None or classif_setup in qut01.data.classif_utils.supported_classif_setups
        ), f"invalid classif setup: {classif_setup}"
        self.classif_setup = classif_setup
        if classif_setup is None:  # same as 'any'
            target_class_count = qut01.data.classif_utils.classif_setup_to_criteria_count_map["any"]
        else:
            target_class_count = qut01.data.classif_utils.classif_setup_to_criteria_count_map[classif_setup]
        assert expected_criteria_count == target_class_count, (
            f"mismatch between classif setup '{classif_setup}' and expected class count {expected_criteria_count}"
            f"\n(should expect {target_class_count} target classes, check config file to make sure model is OK)"
        )
        self.target_class_count = target_class_count
        assert tuple(split_ratios.keys()) == ("train", "valid"), "must specify train+valid, nothing else"
        self.split_ratios = split_ratios
        self.split_seed = split_seed
        self.use_gold_set = use_gold_set
        self._split_statement_ids: typing.Optional[typing.Dict["SubsetType", typing.List[int]]] = None
        self.data_train: typing.Optional[DataParser] = None
        self.data_valid: typing.Optional[DataParser] = None
        self.data_test: typing.Optional[DataParser] = None

    @property
    def num_criteria(self) -> int:
        """Returns the number of target annotation classes (criteria) in the dataset."""
        return self.target_class_count

    def _create_default_parser(self) -> DataParser:
        """Creates and returns a (global) dataset parser with default settings."""
        return DataParser(
            self.data_path,
            dataset_branch=dataset_validated_branch_name,
            verbose=False,
        )

    def prepare_data(self) -> None:
        """Opens and splits the dataset while checking for integrity + annotations."""
        logger.info("Data module preparing underlying dataset with integrity check")
        # note: we do our checks using a temporary object that we do not keep (only setup step does)
        parser = self._create_default_parser()
        creation_date, update_date = parser.info["created_on"], parser.info["updated_on"]
        logger.info(f"dataset created on {creation_date}, last updated on {update_date}")
        self._split_statement_ids = qut01.data.split_utils.get_split_statement_ids(
            data_parser=parser,
            classif_setup=self.classif_setup,
            train_valid_split_ratios=self.split_ratios,
            train_valid_split_seed=self.split_seed,
            use_gold_set=self.use_gold_set,
        )

    def setup(self, stage: typing.Optional[str] = None) -> None:
        """Loads the QUT01-AIMS data under the train/valid/test parsers.

        This method is called by lightning when doing `trainer.fit()` and `trainer.test()`, so be
        careful not to execute the random split twice! The `stage` can be used to differentiate
        whether it's called before `trainer.fit()` or `trainer.test()`.
        """
        if self._split_statement_ids is None:
            logger.warning("should call 'prepare_data()' before 'setup()', avoids per-device call")
            self.prepare_data()
        if any([self.data_train, self.data_valid, self.data_test]):
            logger.info("Data module Setup already done, skipping")
            return
        for subset_type in ["train", "valid", "test"]:
            subset_sids = self._split_statement_ids[subset_type]
            subset_parser = self._get_subset_parser(subset_sids, subset_type)
            setattr(self, f"data_{subset_type}", subset_parser)

    @property
    def split_statement_ids(self) -> typing.Dict["SubsetType", typing.List[int]]:
        """Returns a copy of the dictionary that splits statements by ID across all subsets."""
        if self._split_statement_ids is None:
            self.prepare_data()
        return copy.deepcopy(self._split_statement_ids)

    @property
    def train_statement_ids(self) -> typing.List[int]:
        """Returns a copy of the list of statement identifiers contained in the training set."""
        return self.split_statement_ids["train"]

    @property
    def valid_statement_ids(self) -> typing.List[int]:
        """Returns a copy of the list of statement identifiers contained in the validation set."""
        return self.split_statement_ids["valid"]

    @property
    def test_statement_ids(self) -> typing.List[int]:
        """Returns a copy of the list of statement identifiers contained in the test set."""
        return self.split_statement_ids["test"]

    @property
    def gold_statement_ids(self) -> typing.List[int]:
        """Returns a copy of the list of statement identifiers contained in the gold set.

        Note: these statement identifiers can ALSO be found in the other sets (likely in the test
        set, maybe in the validation set).
        """
        if self.split_statement_ids is None:
            self.prepare_data()
        return self.split_statement_ids["gold"]

    @property
    def unused_statement_ids(self) -> typing.List[int]:
        """Returns the statement identifiers that are UNUSED across all sets."""
        if self.split_statement_ids is None:
            self.prepare_data()
        return self.split_statement_ids["unused"]

    def _get_subset_parser(
        self,
        subset_sids: typing.List[int],
        subset_type: str,
    ) -> typing.Union["torch.utils.data.Dataset", "torch.utils.data.IterableDataset"]:
        """Creates and returns a data subset parser which can be used in a dataloader."""
        default_parser = self._create_default_parser()
        target_meta_name = self.classif_setup
        if target_meta_name == "any":
            target_annot_names = qut01.data.classif_utils.ANNOT_CLASS_NAMES
        elif target_meta_name in qut01.data.classif_utils.ANNOT_CLASS_NAMES:
            target_annot_names = [target_meta_name]
            target_meta_name = qut01.data.classif_utils.ANNOT_CLASS_NAME_TO_META_CLASS_NAME[target_meta_name]
        else:
            assert target_meta_name in qut01.data.classif_utils.ANNOT_META_CLASS_NAMES
            target_annot_names = qut01.data.classif_utils.ANNOT_META_CLASS_NAME_TO_CLASS_NAMES_MAP[target_meta_name]
        kept_sids, kept_gold_sids = [], []

        def check_if_statement_in_subset_and_annotated(sample: deeplake.Dataset) -> bool:
            sid = sample.statement_id.numpy().item()
            # if this statement is not included in the current subset, skip it
            if sid not in subset_sids:
                return False
            # otherwise, if this statement does not have any (relevant) annotation, skip it
            annot_counts = default_parser.get_potential_annotation_counts(sample)  # noqa
            if target_meta_name == "any":
                might_have_annots = any([count > 0 for count in annot_counts.values()])
            else:
                might_have_annots = annot_counts[target_meta_name] > 0
            if not might_have_annots:
                return False
            # finally, if this statement is in the gold set, keep it only if all its annots are validated
            # (i.e. they have a 'last update' attribute set to something)
            if sid not in self.gold_statement_ids:
                kept_sids.append(sid)
                return True
            valid_flags = default_parser.get_validated_annotation_flags(sample)  # noqa
            all_validated = all([valid_flags[annot_name] for annot_name in target_annot_names])
            if all_validated:
                kept_sids.append(sid)
                kept_gold_sids.append(sid)
            return all_validated

        subset = default_parser.dataset.filter(check_if_statement_in_subset_and_annotated, progressbar=False)
        assert len(subset) <= len(subset_sids) and len(subset) == len(kept_sids)
        logger.info(
            f"{subset_type} set has {len(subset)} usable statements with {len(kept_gold_sids)} from the gold set"
        )
        subset_parser = hydra.utils.instantiate(
            # this will call the dataset parser constructor with the dataset subset view
            self._get_subconfig_for_subset(self.dataparser_configs, subset_type),
            subset,
        )
        return subset_parser

    def train_dataloader(self) -> "torch.utils.data.DataLoader":
        """Returns the QUT01-AIMS training set data loader."""
        assert self.data_train is not None, "parser unavailable, call `setup()` first!"
        return self._create_dataloader(self.data_train, subset_type="train")

    def val_dataloader(self) -> "torch.utils.data.DataLoader":
        """Returns the QUT01-AIMS validation set data loader."""
        assert self.data_valid is not None, "parser unavailable, call `setup()` first!"
        return self._create_dataloader(self.data_valid, subset_type="valid")

    def test_dataloader(self) -> "torch.utils.data.DataLoader":
        """Returns the QUT01-AIMS testing set data loader."""
        assert self.data_test is not None, "parser unavailable, call `setup()` first!"
        return self._create_dataloader(self.data_test, subset_type="test")


def _local_main(config) -> None:
    import hydra.utils

    datamodule = hydra.utils.instantiate(config.data.datamodule)
    datamodule.prepare_data()
    datamodule.setup()
    dataloader = datamodule.train_dataloader()
    logger.info(f"dataloader statement count = {len(datamodule.train_statement_ids)}")
    batch = next(iter(dataloader))
    logger.info(f"batch keys: {batch.keys()}")
    logger.info(f"batch id: {batch[qut01.data.batch_id_key]}")
    logger.info(f"batch size: {batch[qut01.data.batch_size_key]}")


if __name__ == "__main__":
    import qut01.utils.config

    qut01.utils.logging.setup_logging_for_analysis_script()
    config_ = qut01.utils.config.init_hydra_and_compose_config(
        overrides=["data=statement_sampler.yaml"],
    )
    _local_main(config_)
