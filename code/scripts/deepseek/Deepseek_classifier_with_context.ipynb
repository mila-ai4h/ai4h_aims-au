{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f95b11c07b3986",
   "metadata": {},
   "source": [
    "# Testing DeepSeek-R1 on UK/CA/AU Statements with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446b09a65015e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torchmetrics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef85243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the annotated data to crete context for the target sentance\n",
    "csv_path = \"data/contextualized_data/AU_context0.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Rename the column names to be consistent with AU\n",
    "if \"sentence_statement_id\" in df.columns and \"statement_id\" not in df.columns:\n",
    "    df.rename(columns={\"sentence_statement_id\": \"statement_id\"}, inplace=True)\n",
    "if \"sentence_orig_idxs\" in df.columns and \"sentence_id\" not in df.columns:\n",
    "    df.rename(columns={\"sentence_orig_idxs\": \"sentence_id\"}, inplace=True)\n",
    "if \"sentence_orig_text\" in df.columns and \"sentence_text\" not in df.columns:\n",
    "    df.rename(columns={\"sentence_orig_text\": \"sentence_text\"}, inplace=True)\n",
    "if \"sentence\" in df.columns and \"sentence_text\" not in df.columns:\n",
    "    df.rename(columns={\"sentence\": \"sentence_text\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7220a4ec-a763-48fd-a87e-6dfbd2f95a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the 'sentence_sentence_statement_id' column to remove the 'SID_' prefix\n",
    "# Check if statement_id is string and contains 'SID_'\n",
    "if isinstance(df[\"statement_id\"], str) and df[\"statement_id\"].str.contains(\"SID_\").any():\n",
    "    df[\"statement_id\"] = df[\"statement_id\"].str.replace(\"SID_\", \"\", regex=False)\n",
    "\n",
    "# Display the first few rows to confirm the change\n",
    "print(df.head())\n",
    "\n",
    "# This is how many words of context (left/right) to take.\n",
    "CONTEXT_SIZE = 50\n",
    "\n",
    "# Group by sentence_statement_id so we can handle each statement independently.\n",
    "grouped = df.groupby(\"statement_id\", group_keys=False)\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "for sentence_statement_id, subdf in grouped:\n",
    "    # Sort the rows in the correct reading order (by sentence_orig_idxs, presumably ascending):\n",
    "    subdf_sorted = subdf.sort_values(\"sentence_id\").reset_index(drop=True)\n",
    "\n",
    "    # 1. Build one flat list of all words for this statement\n",
    "    all_words = []\n",
    "    offsets = []  # will store (start_offset, end_offset) for each row’s sentence\n",
    "    current_offset = 0\n",
    "\n",
    "    for text in subdf_sorted[\"sentence_text\"]:\n",
    "        words = text.split()\n",
    "        start_offset = current_offset\n",
    "        end_offset = start_offset + len(words)\n",
    "        offsets.append((start_offset, end_offset))\n",
    "        all_words.extend(words)\n",
    "        current_offset = end_offset\n",
    "\n",
    "    # 2. Now assign context to each row\n",
    "    for i in range(len(subdf_sorted)):\n",
    "        start_off, end_off = offsets[i]\n",
    "\n",
    "        # Clip the 50 words before/after to the statement boundaries\n",
    "        context_start = max(0, start_off - CONTEXT_SIZE)\n",
    "        context_end = min(len(all_words), end_off + CONTEXT_SIZE)\n",
    "\n",
    "        context_words = all_words[context_start:context_end]\n",
    "        text_with_context = \" \".join(context_words)\n",
    "\n",
    "        # Store it in a new column:\n",
    "        subdf_sorted.loc[i, \"text_with_context\"] = text_with_context\n",
    "\n",
    "    # After we’ve built this for every row in subdf_sorted, collect results\n",
    "    all_outputs.append(subdf_sorted)\n",
    "\n",
    "# 3. Concatenate all statement sub-dataframes into one final DF\n",
    "df_with_context = pd.concat(all_outputs, ignore_index=True)\n",
    "\n",
    "# (Optional) Save to a CSV to inspect\n",
    "df_with_context.to_csv(\"output_with_context.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172787ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Need to launch the deepseek server before running this script\n",
    "Follow the instructions on unsloth deepseek documentation for setup and download of the models\n",
    "https://huggingface.co/unsloth/DeepSeek-R1-GGUF\n",
    "Then, run the following command on the same machine as you are running this script\n",
    "\n",
    "\"podman run -v /network/projects/amlrt/qut01-aims/HF_HOME/DeepSeek-R1-GGUF:/models \\\n",
    "--rm --device nvidia.com/gpu=all \\\n",
    "-p 8080:8080 \\\n",
    "ghcr.io/ggerganov/llama.cpp:full-cuda \\\n",
    "--server -m /models/DeepSeek-R1-UD-Q2_K_XL/DeepSeek-R1-UD-Q2_K_XL-00001-of-00005.gguf \\\n",
    "--ctx-size 8192 --temp 0.6 --threads 16 --n-gpu-layers 62 --cache-type-k q4_0 \\\n",
    "--prio 2 --host 0.0.0.0 --seed 3407\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45c1057-aaed-444a-a131-264aac9d0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_SERVER_URL = \"http://localhost:8080/v1/chat/completions\"\n",
    "\n",
    "punct_remover = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "no_regex = re.compile(\"no[^0-9a-zA-Z]\")\n",
    "yes_regex = re.compile(\"yes[^0-9a-zA-Z]\")\n",
    "\n",
    "model_path = \"/models/DeepSeek-R1-UD-Q2_K_XL/DeepSeek-R1-UD-Q2_K_XL-00001-of-00005.gguf\"\n",
    "prompt = \"\"\"\"<｜User｜>You are an analyst that inspects modern slavery declarations made by Australian reporting\n",
    "entities. You are specialized in the analysis of statements made with respect to the Australian Modern Slavery Act of 2018, and not of any other legislation. You are currently looking for sentences in statements that describe whether the statement is approved by the principal governing body of one or more reporting entities. This governing body holds the primary responsibility for governance of the reporting entity. The Act explicitly prohibits the delegation of this approval authority to individuals, executive committees, sub-committees, or workgroups. It is crucial for entities to clearly identify that the approval has come directly from this sole governing body without ambiguity. Terms like “Executive Leadership Committee,” “Members of the Board,” or “Senior Executive on behalf of the Board” do not sufficiently demonstrate that the principal governing body itself has approved the modern slavery statement. Additionally, descriptions such as \"considered by the board\" or \"put forward to the board\" are also inadequate to fulfill the requirement of direct approval by the governing body. If there is only a single reporting entity, the approval must come from its sole principal governing body.\n",
    "Otherwise, for joint statements made by multiple reporting entities, there are three options for the approval of a statements:\n",
    "The principal governing body of each reporting entity covered by the statement approves the statement. The principal governing body of a higher entity (such as a global parent), which is in a position to influence or control each entity covered by the statement, approves the statement. The higher entity does not have to be a reporting entity itself. If it is not practicable to comply with other options, the principal governing body of at least one reporting entity covered by the statement may approve the statement. In this case, the statement must also explain why this option was taken. We therefore consider any sentence containing language that provides such approval as relevant. Given the above definitions of what constitutes a relevant sentence, you will need to determine if a target sentence is relevant or not inside a larger block of text. The target sentence will first be provided by itself so you can know which sentence we want to classify. It will then be provided again as part of the larger block of text it originally came from (extracted from a PDF file) so you can analyze it with more context. While some of the surrounding sentences may be relevant according to the earlier definitions, we are only interested in classifying the target sentence according to the relevance of its own content. You must avoid labeling sentences with only vague descriptions or corporate talk (and no actual information) as relevant.\n",
    "The answer you provide regarding whether the sentence is relevant or not can only be 'YES' or 'NO', and nothing else. The target sentence to classify is the following:\n",
    "------------\n",
    "{}\n",
    "------------\n",
    "The same target sentence inside its original block of text:\n",
    "------------\n",
    "{}\n",
    "------------\n",
    "Question: Is the target sentence relevant? (YES/NO)<｜Assistant｜>\"\"\"\n",
    "\n",
    "\n",
    "def run_deepseek(target_sentence, text, debug=False):\n",
    "    \"\"\"Runs DeepSeek for a given prompt and returns the response.\"\"\"\n",
    "    current_prompt = prompt.format(target_sentence, text)\n",
    "    max_tokens = 1000\n",
    "    temperature = 0.6\n",
    "    payload = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": current_prompt}],\n",
    "        \"n_predict\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Convert the payload dictionary to a JSON string\n",
    "        response = requests.post(LLAMA_SERVER_URL, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        # Extract the 'content' field from the response (adjust if necessary)\n",
    "        # result = response.json().get(\"content\", \"No response\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error querying Llama: {e}\")\n",
    "        result = None\n",
    "\n",
    "    # Extract final answer\n",
    "    final_answer_match = re.search(r\"\\s*(YES|NO)\\s*$\", result, re.IGNORECASE)\n",
    "\n",
    "    final_answer = final_answer_match.group(1).strip() if final_answer_match else \"Final Answer not found\"\n",
    "\n",
    "    # If the final answer is not found, then look for yes/no at the end of result\n",
    "    if final_answer == \"Final Answer not found\":\n",
    "        final_answer_match = re.search(r\"(yes|no)$\", result[:-20], re.IGNORECASE)\n",
    "        final_answer = final_answer_match.group(1).strip() if final_answer_match else \"Final Answer not found\"\n",
    "\n",
    "    # Normalize the answer\n",
    "    normalized_answer = final_answer.lower().translate(punct_remover).strip()\n",
    "    if normalized_answer == \"no\" or no_regex.match(normalized_answer):\n",
    "        final_result = 0\n",
    "    elif normalized_answer == \"yes\" or yes_regex.match(normalized_answer):\n",
    "        final_result = 1\n",
    "    else:\n",
    "        final_result = -1\n",
    "        if debug:\n",
    "            print(f\"GPT result '{result}' could not be processed (normalized: '{normalized_answer}')\")\n",
    "\n",
    "    if debug:\n",
    "        print(\"Prompt: \", current_prompt)\n",
    "        print(\"XXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "        print(\"This is the model output:\\n\", result)\n",
    "        print(\"XXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "        print(\"Final result: \", final_result)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    return final_result, result, normalized_answer\n",
    "\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e10166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small test to check if everything is working as expected\n",
    "target_sentence = \"Baby Bunting Modern Slavery Statement 2020 This statement, pursuant to the Modern Slavery Act 2018 (Cth), describes the risks of modern slavery in the operations and supply chains of Baby Bunting1 and includes information about actions taken to address those risks for the financial year ended 28 June 2020\"\n",
    "text = \"Baby Bunting Modern Slavery Statement 2020 This statement, pursuant to the Modern Slavery Act 2018 (Cth), describes the risks of modern slavery in the operations and supply chains of Baby Bunting1 and includes information about actions taken to address those risks for the financial year ended 28 June 2020 . Modern slavery includes trafficking in persons, slavery, servitude, forced marriage, forced labour, debt bondage, deceptive recruitment for labour or services; and the worst forms of child labour. Modern slavery has severe consequences for its victims and often disproportionately impacts women and girls. Minimising the risk of modern slavery in its supply chains and, in particular, ensuring that women and girls can exercise their own choices free from the undue influence that arises in modern slavery is a critical focus for Baby Bunting. This is Baby Buntings first modern slavery statement. During the financial year, Baby Bunting has introduced a\"\n",
    "\n",
    "cp = run_deepseek(target_sentence, text, debug=True)\n",
    "print(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda7665f-ba1e-45d7-8f50-13f3c2d2daaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "csv_path = \"output_with_context.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b4162-a498-45f1-8da4-37ed0c684fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = df[\"targets\"].tolist()  # Extract the targets as a list\n",
    "\n",
    "# Manually specify the index for your class of interest\n",
    "class_of_interest_index = 0  # Replace with the index of your desired class\n",
    "\n",
    "class_names = [\n",
    "    \"approval\",\n",
    "    \"signature\",\n",
    "    \"criterion1\",\n",
    "    \"criterion2_structure\",\n",
    "    \"criterion2_operations\",\n",
    "    \"criterion2_supplychains\",\n",
    "    \"criterion3_risks\",\n",
    "    \"criterion4_mitigation\",\n",
    "    \"criterion4_remediation\",\n",
    "    \"criterion5_assessment\",\n",
    "    \"criterion6_consultation\",\n",
    "]\n",
    "# Retrieve the class name corresponding to the index\n",
    "class_of_interest = class_names[class_of_interest_index]\n",
    "\n",
    "# Output results\n",
    "# print(f\"Targets: {targets}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "print(f\"Class index: {class_of_interest_index}\")\n",
    "print(f\"Class name for index {class_of_interest_index} is {class_of_interest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325207ae-7816-4d07-aa8e-bc9e475a1884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters and storage\n",
    "import os\n",
    "\n",
    "count_sentence_was_annotated_with_class_of_interest = 0\n",
    "count_sentence_was_not_annotated_with_class_of_interest = 0\n",
    "data = []\n",
    "already_there = 0\n",
    "annotated = 0\n",
    "not_ok_results = 0\n",
    "not_ok_details = []\n",
    "tot_count = 0\n",
    "\n",
    "# Parameters\n",
    "max_amount = 9999  # Limit the computation\n",
    "print_count = True\n",
    "inter_query_wait_time_in_sec = 2  # Time to wait between queries\n",
    "debug = False\n",
    "\n",
    "# Load cached results if available\n",
    "prev_df = (\n",
    "    pd.read_csv(f\"result_for_class_index_test{class_of_interest_index}.csv\")\n",
    "    if os.path.isfile(f\"result_for_class_index_test{class_of_interest_index}.csv\")\n",
    "    else None\n",
    ")  # Replace with pandas.read_csv if using a cached file\n",
    "predict_fun = run_deepseek  # Replace with the appropriate prediction function\n",
    "\n",
    "try:\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        tot_count += 1\n",
    "        sentence_text = row[\"sentence_text\"]\n",
    "        sentence_statement_id = int(row[\"statement_id\"])\n",
    "        sentence_orig_idxs = int(row[\"sentence_id\"])\n",
    "        text_with_context = row[\"text_with_context\"]\n",
    "\n",
    "        # Check if the sentence is already cached\n",
    "        if (\n",
    "            prev_df is not None\n",
    "            and (\n",
    "                (prev_df[\"sentence_statement_id\"] == sentence_statement_id)\n",
    "                & (prev_df[\"sentence_orig_idxs\"] == sentence_orig_idxs)\n",
    "            ).any()\n",
    "        ):\n",
    "            # Retrieve cached result\n",
    "            cached_row = prev_df[\n",
    "                (prev_df[\"sentence_statement_id\"] == sentence_statement_id)\n",
    "                & (prev_df[\"sentence_orig_idxs\"] == sentence_orig_idxs)\n",
    "            ]\n",
    "            if len(cached_row) != 1:\n",
    "                raise ValueError(f\"Multiple cache rows found for: {sentence_statement_id}:{sentence_orig_idxs}\")\n",
    "\n",
    "            cached_row = cached_row.iloc[0]\n",
    "            data.append(\n",
    "                [\n",
    "                    sentence_statement_id,\n",
    "                    sentence_orig_idxs,\n",
    "                    cached_row.get(\"target_classes\", []),\n",
    "                    cached_row.get(\"predicted_classes\", []),\n",
    "                    cached_row.get(\"reasoning\", \"\"),\n",
    "                    cached_row.get(\"answer\", \"\"),\n",
    "                ]\n",
    "            )\n",
    "            already_there += 1\n",
    "\n",
    "        else:\n",
    "            # Process new sentence\n",
    "            target_classes = [int(x) for x in eval(row[\"targets\"])]  # Parse targets correctly\n",
    "            predicted_classes = [-1] * len(target_classes)  # Initialize predictions\n",
    "            reasoning = \"\"\n",
    "            answer = \"\"\n",
    "\n",
    "            if target_classes[class_of_interest_index] > -1:\n",
    "                # Predict classification\n",
    "                predicted_class, reasoning, answer = predict_fun(sentence_text, text_with_context, debug=debug)\n",
    "                time.sleep(inter_query_wait_time_in_sec)\n",
    "\n",
    "                predicted_classes[class_of_interest_index] = predicted_class\n",
    "                count_sentence_was_annotated_with_class_of_interest += 1\n",
    "\n",
    "                # Handle problematic results\n",
    "                if predicted_class == -1:\n",
    "                    not_ok_results += 1\n",
    "                    not_ok_details.append([sentence_statement_id, sentence_orig_idxs, reasoning, answer])\n",
    "\n",
    "            else:\n",
    "                count_sentence_was_not_annotated_with_class_of_interest += 1\n",
    "\n",
    "            # Append processed row\n",
    "            data.append(\n",
    "                [sentence_statement_id, sentence_orig_idxs, target_classes, predicted_classes, reasoning, answer]\n",
    "            )\n",
    "            annotated += 1\n",
    "\n",
    "        # Stop processing if the max limit is reached\n",
    "        if max_amount > -1 and tot_count >= max_amount:\n",
    "            print(f\"Reached the max amount of {max_amount}\")\n",
    "            break\n",
    "\n",
    "        # Print progress\n",
    "        if print_count:\n",
    "            print(f\"Processed {tot_count} / {max_amount}\")\n",
    "\n",
    "        if annotated % 25 == 0:\n",
    "            print(f\"Annotated {annotated} sentences\")\n",
    "            result_df = pd.DataFrame(\n",
    "                data,\n",
    "                columns=[\n",
    "                    \"sentence_statement_id\",\n",
    "                    \"sentence_orig_idxs\",\n",
    "                    \"target_classes\",\n",
    "                    \"predicted_classes\",\n",
    "                    \"reasoning\",\n",
    "                    \"answer\",\n",
    "                ],\n",
    "            )\n",
    "            result_df.to_csv(f\"result_for_class_index_test{class_of_interest_index}.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    # Save results to CSV\n",
    "    result_df = pd.DataFrame(\n",
    "        data,\n",
    "        columns=[\n",
    "            \"sentence_statement_id\",\n",
    "            \"sentence_orig_idxs\",\n",
    "            \"target_classes\",\n",
    "            \"predicted_classes\",\n",
    "            \"reasoning\",\n",
    "            \"answer\",\n",
    "        ],\n",
    "    )\n",
    "    result_df.to_csv(f\"result_for_class_index_test{class_of_interest_index}.csv\", index=False)\n",
    "\n",
    "    # Print summary\n",
    "    print(\n",
    "        f\"{already_there} sentences retrieved from cache. {annotated} sentences were newly annotated.\\n\"\n",
    "        f\"Of these, {count_sentence_was_annotated_with_class_of_interest} were annotated with the class of interest, \"\n",
    "        f\"and {count_sentence_was_not_annotated_with_class_of_interest} were not.\\n\"\n",
    "        f\"Total processed: {count_sentence_was_annotated_with_class_of_interest + count_sentence_was_not_annotated_with_class_of_interest}\\n\"\n",
    "        f\"Problematic results: {not_ok_results}\"\n",
    "    )\n",
    "\n",
    "    # Save problematic results\n",
    "    if not_ok_details:\n",
    "        not_ok_df = pd.DataFrame(not_ok_details, columns=[\"statement_id\", \"sentence_orig_idxs\", \"reasoning\", \"answer\"])\n",
    "        not_ok_df.to_csv(f\"not_ok_for_class_index_{class_of_interest_index}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899fe4f1-805a-4526-befe-43aaf07b84db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data Length: {len(data)}\")\n",
    "print(f\"Example Row Lengths: {[len(row) for row in data[:5]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa5ed2-5f32-43a3-b388-c4d659753f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"result_for_class_index_test{class_of_interest_index}.csv\")\n",
    "amount_of_classes = 11\n",
    "\n",
    "\n",
    "def parse_list(list_as_str):\n",
    "    stripped = list_as_str.strip(\"[]\")\n",
    "    lst = [int(x) for x in stripped.split(\",\")]\n",
    "    assert len(lst) == amount_of_classes\n",
    "    return lst\n",
    "\n",
    "\n",
    "preds = [parse_list(e)[class_of_interest_index] for e in list(df[\"predicted_classes\"])]\n",
    "targets = [parse_list(e)[class_of_interest_index] for e in list(df[\"target_classes\"])]\n",
    "stat_ids = df[\"sentence_statement_id\"]\n",
    "sent_ids = df[\"sentence_orig_idxs\"]\n",
    "assert len(preds) == len(targets) == len(stat_ids) == len(sent_ids)\n",
    "\n",
    "# note: preds are casted to 0 when they are -1, otherwise torchmertric will complain\n",
    "# this is ok because we still keep the target with the -1, so they will be ignored\n",
    "# Corrected version\n",
    "fixed_preds = [x if x > -1 else 0 for x in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893cd86f-fbf3-4323-87e8-50ec9ac4d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_fun = torchmetrics.classification.F1Score(task=\"binary\", ignore_index=-1)\n",
    "f1 = f1_fun(torch.tensor(fixed_preds), torch.tensor(targets))\n",
    "p_fun = torchmetrics.classification.Precision(task=\"binary\", ignore_index=-1)\n",
    "p = p_fun(torch.tensor(fixed_preds), torch.tensor(targets))\n",
    "r_fun = torchmetrics.classification.Recall(task=\"binary\", ignore_index=-1)\n",
    "r = r_fun(torch.tensor(fixed_preds), torch.tensor(targets))\n",
    "\n",
    "acc_fun = torchmetrics.classification.Accuracy(task=\"binary\", ignore_index=-1)\n",
    "acc = acc_fun(torch.tensor(fixed_preds), torch.tensor(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadd9484-5ac7-4ece-8fce-66ee1703d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"example amount is {len(preds)}\")\n",
    "print(f\"precision is {p:.3f}, recall is {r:.3f}, f1 is {f1:.3f}\")\n",
    "print(f\"accuracy is {acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qut02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
