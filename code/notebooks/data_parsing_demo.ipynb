{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d441d7f42a30a50b",
   "metadata": {},
   "source": [
    "# AIMS.au Statement Dataset Parsing Demo\n",
    "\n",
    "This notebook shows how to parse the statements dataset that should have been created with the\n",
    "scripts located in [this folder](../qut01/data/scripts). To get a already-generated copy of the\n",
    "dataset, check out the links in the top-level README.\n",
    "\n",
    "Note: the dataset is currently packaged using deeplake. That means it looks like a folder\n",
    "of not-so-easy-to-understand data when we look at it on disk, but that's because the contents are\n",
    "chunked and compressed and stored in order to simplify the data loading process for future\n",
    "experiments. See [this page](https://docs.deeplake.ai/en/latest/Datasets.html) for API\n",
    "documentation on the deeplake dataset format.\n",
    "\n",
    "This notebook was last updated on 2024-07-31 for framework v0.5.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df941cdd28aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import qut01\n",
    "\n",
    "qut01.utils.logging.setup_logging_for_analysis_script()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a309ea4765f8a4e",
   "metadata": {},
   "source": [
    "### Opening the dataset\n",
    "\n",
    "Note: before we actually fetch the data for a particular statement, it is not guaranteed that it will\n",
    "contain a specific number of annotations, even if annotators should have provided labels for it. This\n",
    "is because some annotations are discarded when parsed (due to bad formatting or other labeling issues),\n",
    "and others are replaced by a single \"validated\" annotation so that the statement becomes part of\n",
    "the gold evaluation subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d64ae9354c850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this path to something else if you are storing the dataset in the non-default location!\n",
    "try:\n",
    "    dataset_path = qut01.data.dataset_parser.get_default_deeplake_dataset_path()\n",
    "    print(f\"Will try to open the statements dataset at: {dataset_path}\")\n",
    "    assert dataset_path.exists()\n",
    "except:  # noqa\n",
    "    raise RuntimeError(\n",
    "        \"dataset not found!\"\n",
    "        \"\\n\\t...you must define a .env file with a proper DATA_ROOT variable, or hard-code the path here!\"\n",
    "        \"\\n\\t   (see the README.md and .env.template files for more details on how to use it)\"\n",
    "    )\n",
    "\n",
    "dataset = qut01.data.dataset_parser.get_deeplake_dataset(  # this will load the deeplake dataset itself\n",
    "    dataset_path=dataset_path,\n",
    "    checkout_branch=qut01.data.dataset_parser.dataset_validated_branch_name,  # to load all annotations (train-valid-test)\n",
    ")\n",
    "data_parser = qut01.data.dataset_parser.DataParser(  # this will give us a easy-to-use parser for the dataset\n",
    "    dataset_path_or_object=dataset,\n",
    "    use_processed_data_cache=False,  # we will iterate over the entire dataset below, caching might go out of memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa87c750dba39055",
   "metadata": {},
   "source": [
    "All of the dataset columns should have been printed above; if you would like to access those\n",
    "directly, you can use the `dataset` object we just created almost like a pandas dataframe. Refer to\n",
    "the deeplake API documentation for more information on deeplake dataset objects.\n",
    "\n",
    "Now, let's display some high-level statistics that do not require iterating through all the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13610f0fcef2425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'potentially' annotated statements correspond to statements that should have been annotated by workers\n",
    "potentially_annotated_statement_ids = data_parser.get_potentially_annotated_statement_ids()\n",
    "all_annot_sids = set([sid for annot_sids in potentially_annotated_statement_ids.values() for sid in annot_sids])\n",
    "print(f\"\\ndataset contains {len(data_parser)} statements (and {len(all_annot_sids)} with annotations)\")\n",
    "for annot_type, annot_sids in potentially_annotated_statement_ids.items():\n",
    "    print(f\"\\tthere are {len(annot_sids)} annotated statements for {annot_type}\")\n",
    "annotated_statement_flags = [sid in all_annot_sids for sid in dataset.statement_id]\n",
    "# the publication dates correspond to the date when the statement was made available online\n",
    "publication_dates = [\n",
    "    datetime.datetime.strptime(t.item(), \"%Y-%m-%d %H:%M:%S\") for t in dataset.metadata.PublishedAt.numpy()\n",
    "]\n",
    "print(f\"{min(publication_dates).isoformat()=}\")\n",
    "print(f\"{max(publication_dates).isoformat()=}\")\n",
    "# the statement period end corresponds to the end of the financial year covered by the statement\n",
    "period_end_dates = [datetime.datetime.strptime(t.item(), \"%Y-%m-%d\") for t in dataset.metadata.PeriodEnd.numpy()]\n",
    "print(f\"{min(period_end_dates).isoformat()=}\")\n",
    "print(f\"{max(period_end_dates).isoformat()=}\")\n",
    "# we can compute aggregate statistics for the metadata fields directly using the dataset object\n",
    "print(f\"total page count: {dataset.metadata.PageCount.numpy().sum()}\")\n",
    "print(f\"total word count: {dataset.metadata.WordCount.numpy().sum()}\")\n",
    "# using the binary flags indicating whether statements were annotated, we can compute stats for annotated statements\n",
    "print(f\"annotated page count: {dataset[annotated_statement_flags].metadata.PageCount.numpy().sum()}\")\n",
    "print(f\"annotated word count: {dataset[annotated_statement_flags].metadata.WordCount.numpy().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ab524c71056182",
   "metadata": {},
   "source": [
    "### Reading the data from the dataset for a random statement\n",
    "\n",
    "Note: the statement INDEX is not the same as its IDENTIFIER; the index is a 0-based integer that identifies the\n",
    "statement in the dataset itself, whereas the identifier is an arbitrary integer that identifies the statement\n",
    "on the modern slavery register.\n",
    "\n",
    "Below, we pick a random statement by index, and then convert that index to the statement's identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0afc503753ba808",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_statement_idx = np.random.randint(len(data_parser))\n",
    "statement_data = data_parser[random_statement_idx]\n",
    "statement_id = statement_data[\"statement_id\"]\n",
    "print(f\"displaying data for statement ID={statement_id}:\")\n",
    "\n",
    "# another way to get the id from the index without fetching the data is to use a data parser attribute:\n",
    "expected_statement_id = data_parser.statement_ids[random_statement_idx]\n",
    "assert expected_statement_id == statement_id\n",
    "\n",
    "# some of the 'raw' tensor data can be directly accessed via the loaded batch dictionary:\n",
    "print(f\"\\tlink: {statement_data['metadata/Link'].item()}\")\n",
    "print(f\"\\tpage count: {statement_data['metadata/PageCount'].item()}\")\n",
    "print(f\"\\tpdf size: {statement_data['pdf_data'].size} bytes\")\n",
    "\n",
    "# metadata that requires a bit of processing can be queried through the data parser:\n",
    "annot_counts = data_parser.get_potential_annotation_counts(statement_data)\n",
    "for annot_type, annot_count in annot_counts.items():\n",
    "    print(f\"\\tnumber of potential {annot_type} annotations: {annot_count}\")\n",
    "valid_flags = data_parser.get_validated_annotation_flags(statement_data)\n",
    "for annot_type, valid_flag in valid_flags.items():\n",
    "    if valid_flag:\n",
    "        print(f\"\\tannotation '{annot_type}' has been validated\")\n",
    "\n",
    "# processed text and annotations are provided on-demand via a new dataclass:\n",
    "statement_processed_data = data_parser.get_processed_data(random_statement_idx, statement_data)\n",
    "print(f\"\\tprocessed sentence count: {len(statement_processed_data.sentences)}\")\n",
    "print(f\"\\tannotation count: {len(statement_processed_data.annotations)}\")\n",
    "if len(statement_processed_data.annotations):\n",
    "    print(f\"\\t\\t{len(statement_processed_data.annotation_chunks)} annotation chunks\")\n",
    "    print(f\"\\t\\t{len(statement_processed_data.annotated_sentence_idxs)} sentences matched to at least one annotation\")\n",
    "    print(f\"\\t\\t{len(statement_processed_data.unannotated_sentence_idxs)} sentences matched to no annotation\")\n",
    "    if statement_processed_data.annotation_chunks:\n",
    "        chunk = np.random.choice(statement_processed_data.annotation_chunks)\n",
    "        assert isinstance(chunk, qut01.data.annotations.chunks.AnnotationTextChunk)\n",
    "        print(\"\\trandomly picked annotation chunk:\")\n",
    "        print(f\"\\t\\ttype: {chunk.annotation.name}\")\n",
    "        print(f\"\\t\\tlabel: {chunk.annotation.label.name}\")\n",
    "        print(f\"\\t\\tsentence match scores: {chunk.matched_sentences_scores}\")\n",
    "        for sentence_idx, sentence in enumerate(chunk.sentences):\n",
    "            print(f\"\\t\\tsentence {sentence_idx + 1}/{len(chunk.sentences)}:  {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ad9cb0050b1f41",
   "metadata": {},
   "source": [
    "### Loading specific annotations using the framework's annotation parsing objects:\n",
    "\n",
    "(i.e. without having to read individual statements using the above data parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c09ca2bff915e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qut01.data.annotations.classes\n",
    "\n",
    "# first, we'll make sure we read a bunch of statements that possesses all annotations\n",
    "target_statement_ids = [62, 701, 1578, 1583, 1702, 1880, 1907, 2407, 3072]  # from 2024-01-28 sample\n",
    "target_annot_types = [\n",
    "    qut01.data.annotations.classes.Criterion2Structure,\n",
    "    qut01.data.annotations.classes.Criterion2Operations,\n",
    "    qut01.data.annotations.classes.Criterion2SupplyChains,\n",
    "    qut01.data.annotations.classes.Criterion3RiskDesc,\n",
    "    qut01.data.annotations.classes.Criterion4Mitigation,\n",
    "    qut01.data.annotations.classes.Criterion4Remediation,\n",
    "    qut01.data.annotations.classes.Criterion5Effect,\n",
    "    qut01.data.annotations.classes.Criterion6Consult,\n",
    "]\n",
    "# we can then use this function to get ALL ANNOTATIONS across all of the targeted statements\n",
    "annotations = qut01.data.annotations.classes.get_annotations(\n",
    "    dataset=data_parser,\n",
    "    target_statement_ids=target_statement_ids,\n",
    "    target_annot_types=target_annot_types,\n",
    ")\n",
    "\n",
    "# finally, we will compute inter-annotator-agreement scores for these annotations\n",
    "iaa_iou_scores = {atype: [] for atype in target_annot_types}  # annot_type-to-score-list\n",
    "for target_sid in target_statement_ids:\n",
    "    for target_annot_type in target_annot_types:\n",
    "        target_annots = [a for a in annotations if a.statement_id == target_sid and type(a) is target_annot_type]\n",
    "        if not target_annots:\n",
    "            continue  # annotations were filtered out due to errors\n",
    "        assert len(target_annots) == 2  # for the annotation types we selected, we should get double-annotations\n",
    "        iaa_iou = qut01.metrics.iaa.compute_inter_annotator_agreement(*target_annots)\n",
    "        print(f\"IAA-IoU for statement_{target_sid}, {target_annot_type.name}: {iaa_iou:.02f}\")\n",
    "        iaa_iou_scores[target_annot_type].append(iaa_iou)\n",
    "print()\n",
    "for target_annot_type in target_annot_types:\n",
    "    scores = iaa_iou_scores[target_annot_type]\n",
    "    iaa_iou = (sum(scores) / len(scores)) if len(scores) else 0.0\n",
    "    print(f\"average IAA-IoU, {target_annot_type.name} ({len(scores)} statements): {iaa_iou:.02f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395ca4ebeca1f0ba",
   "metadata": {},
   "source": [
    "### Parsing all annotated sentences across all statements\n",
    "\n",
    "(i.e. exhaustively, and this will be slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff023e8a278d30c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sentence_counts = []\n",
    "total_relevant_sentence_counts = []\n",
    "for target_sid in tqdm.tqdm(all_annot_sids, desc=\"parsing all annotated statements\"):\n",
    "    target_idx = data_parser.statement_ids.index(target_sid)\n",
    "    statement_processed_data = data_parser.get_processed_data(target_idx)\n",
    "    total_sentence_counts.append(len(statement_processed_data.sentences))\n",
    "    total_relevant_sentence_counts.append(len(statement_processed_data.annotated_sentence_idxs))\n",
    "\n",
    "print(f\"\\ttotal annotated sentence count: {sum(total_sentence_counts)}\")\n",
    "print(f\"\\ttotal relevant sentence count: {sum(total_relevant_sentence_counts)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
