{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import shutil\n",
    "import textwrap\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import qut01\n",
    "\n",
    "qut01.utils.logging.setup_logging_for_analysis_script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506fa339df22d72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = qut01.data.dataset_parser.get_default_deeplake_dataset_path()\n",
    "dataset = qut01.data.dataset_parser.get_deeplake_dataset(  # this will load the deeplake dataset itself\n",
    "    dataset_path=dataset_path,\n",
    "    checkout_branch=qut01.data.dataset_parser.dataset_annotated_branch_name,  # NOTE: not validated data!\n",
    ")\n",
    "data_parser = qut01.data.dataset_parser.DataParser(  # this will give us a easy-to-use parser for the dataset\n",
    "    dataset_path_or_object=dataset,\n",
    "    use_processed_data_cache=False,  # we will iterate over the entire dataset below, caching might go out of memory\n",
    ")\n",
    "potentially_annotated_statement_ids = data_parser.get_potentially_annotated_statement_ids()\n",
    "fully_annot_sids = functools.reduce(set.intersection, [set(v) for v in potentially_annotated_statement_ids.values()])\n",
    "target_sidxs = [data_parser.statement_ids.index(sid) for sid in fully_annot_sids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb654a55240232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "np.random.shuffle(target_sidxs)\n",
    "\n",
    "num_samples = 5\n",
    "output_dir = \"/tmp/raw_data_sample/\"\n",
    "abbyy_data_subdir = os.path.join(output_dir, \"abbyy\")\n",
    "fitz_data_subdir = os.path.join(output_dir, \"fitz\")\n",
    "pdf_data_subdir = os.path.join(output_dir, \"pdf\")\n",
    "annotation_csv_path = os.path.join(output_dir, \"annotations.csv\")\n",
    "metadata_csv_path = os.path.join(output_dir, \"metadata.csv\")\n",
    "readme_path = os.path.join(output_dir, \"README.txt\")\n",
    "\n",
    "if os.path.exists(output_dir) and os.path.isdir(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_annots_df, output_metadata_df = None, None\n",
    "\n",
    "for target_sidx in target_sidxs[:num_samples]:\n",
    "    tensor_data = data_parser.get_tensor_data(target_sidx)\n",
    "    statement_id = int(tensor_data[\"statement_id\"])\n",
    "\n",
    "    annotation_cols = [k for k in tensor_data.keys() if k.startswith(\"annotations/\") and \"annotated\" not in k]\n",
    "    single_annot_cols = [k for k in annotation_cols if \"/a-s-c1/\" in k]\n",
    "    double_annot_cols = [k for k in annotation_cols if \"/c2-c3-c4-c5-c6/\" in k]\n",
    "    metadata_cols = [k for k in tensor_data.keys() if k.startswith(\"metadata/\") and \"LocalLink\" not in k]\n",
    "\n",
    "    single_annots = pd.Series(\n",
    "        {\n",
    "            \"statement_id\": str(statement_id),\n",
    "            **{k.split(\"/\")[-1]: tensor_data[k].item() for k in single_annot_cols},\n",
    "        }\n",
    "    )\n",
    "    double_annots = [\n",
    "        pd.Series(\n",
    "            {\n",
    "                \"statement_id\": str(statement_id),\n",
    "                **{k.split(\"/\")[-1]: tensor_data[k][annotator_idx].item() for k in double_annot_cols},\n",
    "            }\n",
    "        )\n",
    "        for annotator_idx in range(2)\n",
    "    ]\n",
    "\n",
    "    annots_df = pd.concat([single_annots, *double_annots], axis=1)\n",
    "    annots_df = annots_df.transpose()\n",
    "    annots_df.fillna(value=np.nan, inplace=True)\n",
    "\n",
    "    metadata_df = pd.Series(\n",
    "        {\n",
    "            \"statement_id\": str(statement_id),\n",
    "            **{k.split(\"/\")[-1]: tensor_data[k].item() for k in metadata_cols},\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if output_annots_df is None:\n",
    "        output_annots_df = annots_df\n",
    "        output_metadata_df = metadata_df\n",
    "    else:\n",
    "        output_annots_df = pd.concat([output_annots_df, annots_df], axis=0)\n",
    "        output_metadata_df = pd.concat([output_metadata_df, metadata_df], axis=1)\n",
    "\n",
    "    os.makedirs(abbyy_data_subdir, exist_ok=True)\n",
    "    os.makedirs(fitz_data_subdir, exist_ok=True)\n",
    "    os.makedirs(pdf_data_subdir, exist_ok=True)\n",
    "\n",
    "    abbyy_text_path = os.path.join(abbyy_data_subdir, f\"{statement_id}.txt\")\n",
    "    with open(abbyy_text_path, \"w\") as fd:\n",
    "        fd.write(tensor_data[\"abbyy/text\"].item())\n",
    "\n",
    "    fitz_text_path = os.path.join(fitz_data_subdir, f\"{statement_id}.txt\")\n",
    "    with open(fitz_text_path, \"w\") as fd:\n",
    "        fd.write(tensor_data[\"fitz/text\"].item())\n",
    "\n",
    "    pdf_path = os.path.join(pdf_data_subdir, f\"{statement_id}.pdf\")\n",
    "    with open(pdf_path, \"wb\") as fd:\n",
    "        fd.write(tensor_data[\"pdf_data\"])\n",
    "\n",
    "output_annots_df.to_csv(annotation_csv_path)\n",
    "output_metadata_df = output_metadata_df.transpose()\n",
    "output_metadata_df.to_csv(metadata_csv_path)\n",
    "\n",
    "readme_content = f\"\"\"\\\n",
    "This data sample contains the original PDF, extracted text (using ABBYY FineReader and fitz), and annotations for {num_samples} modern slavery statements published on the Australian Modern Slavery Register.\n",
    "\n",
    "The data provided here is formatted in a way to simplify exploration and parsing directly from a file browser. The real dataset contains more metadata and files (e.g. the auxiliary outputs from ABBYY FineReader and fitz), and is packaged in deeplake/HDF5 format.\n",
    "\n",
    "NeurIPS 2024 Datasets and Benchmarks Track, Submission #1041 --- do not redistribute.\n",
    "\"\"\"\n",
    "wrapped_readme_content = \"\\n\\n\".join(\n",
    "    \"\\n\".join(textwrap.wrap(paragraph, width=100)) for paragraph in readme_content.split(\"\\n\\n\")\n",
    ")\n",
    "\n",
    "with open(readme_path, \"w\") as fd:\n",
    "    fd.write(wrapped_readme_content)\n",
    "\n",
    "print(\"all done\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "ebf9e084a48df8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
