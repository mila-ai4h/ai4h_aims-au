{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f95b11c07b3986",
   "metadata": {},
   "source": [
    "# Statement DataModule Analysis\n",
    "\n",
    "This notebook analyzes the data loaded by the statement data module. For a more simple demo showing\n",
    "how to parse the statement dataset, see [this notebook](./data_parsing_demo.ipynb).\n",
    "\n",
    "This notebook was last updated on 2024-04-02 for framework v0.4.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446b09a65015e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "import hydra\n",
    "import lightning.pytorch as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import numpy as np\n",
    "import pandas\n",
    "import torch\n",
    "import torchmetrics\n",
    "import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "import qut01.utils.config\n",
    "import qut01.utils.logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = qut01.utils.logging.setup_logging_for_analysis_script()\n",
    "data_config_name = \"statement_sampler.yaml\"\n",
    "logger.info(f\"initializing hydra and fetching data config for '{data_config_name}'...\")\n",
    "overrides = [\n",
    "    f\"data={data_config_name}\",\n",
    "    \"data.classif_setup=any\",\n",
    "    \"data.num_criteria=11\",\n",
    "]\n",
    "config = qut01.utils.config.init_hydra_and_compose_config(overrides=overrides)\n",
    "logger.info(\"initialization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9041003af0a8a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Instantiating datamodule: {config.data.datamodule._target_}\")  # noqa\n",
    "datamodule: pl.LightningDataModule = hydra.utils.instantiate(config.data.datamodule)\n",
    "assert isinstance(datamodule, pl.LightningDataModule), f\"unexpected type: {type(datamodule)}\"\n",
    "logger.info(\"running 'datamodule.prepare_data()'...\")\n",
    "datamodule.prepare_data()\n",
    "logger.info(\"running 'datamodule.setup()'...\")\n",
    "datamodule.setup(stage=\"fit\")\n",
    "logger.info(\"fetching train data loader...\")\n",
    "dataloader = datamodule.train_dataloader()\n",
    "logger.info(\"train data loader ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e690b-68db-427e-b5d0-4b7027238cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "punct_remover = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "no_regex = re.compile(\"no[^0-9a-zA-Z]\")\n",
    "yes_regex = re.compile(\"yes[^0-9a-zA-Z]\")\n",
    "\n",
    "openai_model = \"gpt-3.5-turbo-0125\"\n",
    "# openai_model = \"gpt-4o\"\n",
    "\n",
    "prompt = \"\"\"You are an analyst that inspects modern slavery declarations made by Australian reporting entities. You are specialized in the analysis of statements made with respect to the Australian Modern Slavery Act of 2018, and not of any other legislation.\n",
    "\n",
    "You are currently looking for sentences in statements that describe the SUPPLY CHAINS of an entity, where supply chains refer to the sequences of processes involved in the procurement of products and services (including labour) that contribute to the reporting entity's own products and services. The description of a supply chain can be related, for example, to 1) the products that are provided by suppliers; 2) the services provided by suppliers, or 3) the location, category, contractual arrangement, or other attributes that describe the suppliers. Any sentence that contains these kinds of information is considered relevant. Descriptions that apply to indirect suppliers (i.e. suppliers-of-suppliers) are considered relevant. Descriptions of the supply chains of entities owned or controlled by the reporting entity making the statement are also considered relevant. However, descriptions of 'downstream' supply chains, i.e. of how customers and clients of the reporting entity use its products or services, are NOT considered relevant. Finally, sentences that describe how the reporting entity lacks information on some of its supply chain, or how some of its supply chains are still unmapped or unidentified, are also considered relevant.\n",
    "\n",
    "Given the above definitions of what constitutes a relevant sentence, you will need to determine if a target sentence is relevant or not. You must avoid labeling sentences with only vague descriptions or corporate talk (and no actual information) as relevant. The answer you provide regarding whether the sentence is relevant or not can only be 'YES' or 'NO', and nothing else.\n",
    "\n",
    "The target sentence to classify is the following:\n",
    "------------\n",
    "{}\n",
    "------------\n",
    "\n",
    "Is the target sentence relevant? (YES/NO)\"\"\"\n",
    "# If YES, tell me why you think it is relevant\"\"\"\n",
    "\n",
    "\n",
    "def gpt_classify(target_sentence, debug=False):\n",
    "\n",
    "    current_prompt = prompt.format(target_sentence)\n",
    "\n",
    "    cp = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": current_prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=openai_model,\n",
    "    )\n",
    "    assert len(cp.model_dump()[\"choices\"]) == 1\n",
    "    result = cp.model_dump()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    massaged_result = result.lower().translate(punct_remover).strip()\n",
    "\n",
    "    if massaged_result == \"no\" or no_regex.match(massaged_result):\n",
    "        final_result = 0\n",
    "    elif massaged_result == \"yes\" or yes_regex.match(massaged_result):\n",
    "        final_result = 1\n",
    "    else:\n",
    "        final_result = -1\n",
    "        print(f\"GPT result '{result}' not ok (after massage, it is '{massaged_result}')\")\n",
    "\n",
    "    if debug:\n",
    "        print(current_prompt)\n",
    "        print(result)\n",
    "        print(final_result)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    return final_result, result, massaged_result\n",
    "\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "target_sentence = \"Baby Bunting Modern Slavery Statement 2020 This statement, pursuant to the Modern Slavery Act 2018 (Cth), describes the risks of modern slavery in the operations and supply chains of Baby Bunting1 and includes information about actions taken to address those risks for the financial year ended 28 June 2020\"\n",
    "\n",
    "# cp = gpt_classify(target_sentence, debug=True)\n",
    "# print(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3875363d-baa1-4c77-8142-41d322d485ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_classify(sentence_text, debug=False):\n",
    "    print(sentence_text)\n",
    "    return -1, \"raw example text\", \"processed example text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ed76e1-d043-4937-95f2-4480cfe08105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_of_interest = \"c2 (supply chains)\"\n",
    "\n",
    "example_item = next(datamodule.train_dataloader().__iter__())\n",
    "class_names = example_item[\"class_names\"]\n",
    "amount_of_classes = len(class_names)\n",
    "class_of_interest_index = class_names.index(class_of_interest)\n",
    "print(f\"class names: {class_names}\")\n",
    "print(f\"class index for {class_of_interest} is {class_of_interest_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19505c99-b5d4-4b1f-8288-0a97913d3e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_sentence_was_annotated_with_class_of_interest = 0\n",
    "count_sentence_was_not_annotated_with_class_of_interest = 0\n",
    "data = []\n",
    "already_there = 0\n",
    "annotated = 0\n",
    "not_ok_results = 0\n",
    "not_ok_details = []\n",
    "tot_count = 0\n",
    "\n",
    "# THESE PARAMETERS SHOULD BE MODIFIED TO REFLECT THE WANTED BVEHAVIOUR\n",
    "max_amount = -1  # 2000  # 1000 # set this to limit the computation\n",
    "print_count = True\n",
    "inter_query_wait_time_in_sec = 0.5  # set to 0 to not wait\n",
    "\n",
    "prev_df = pandas.read_csv(f\"result_for_class_index_{class_of_interest_index}.csv\")\n",
    "# prev_df = None\n",
    "\n",
    "# predict_fun = mock_classify\n",
    "predict_fun = gpt_classify\n",
    "\n",
    "# use debug for GPT\n",
    "debug = False  # True\n",
    "\n",
    "try:  # to handle ChatGPT possible exceptions but still save results\n",
    "    # for item in tqdm.tqdm(itertools.chain(datamodule.train_dataloader(), datamodule.val_dataloader(), datamodule.test_dataloader())):\n",
    "    for item in tqdm.tqdm(datamodule.val_dataloader()):\n",
    "        for i, sentence_text in enumerate(item[\"sentence_orig_text\"]):\n",
    "            tot_count += 1\n",
    "            sentence_statement_id = int(item[\"statement_id\"][i])\n",
    "            sentence_orig_idxs = item[\"sentence_orig_idxs\"][i]\n",
    "            assert len(sentence_orig_idxs) == 1\n",
    "            sentence_orig_idxs = int(sentence_orig_idxs[0])\n",
    "            text_with_context = item[\"text\"][i]\n",
    "            assert (\n",
    "                text_with_context == sentence_text\n",
    "            ), f\"context must be disabled in this experiment. Found '{text_with_context}'\"\n",
    "\n",
    "            if (prev_df is not None) and (\n",
    "                (prev_df[\"sentence_statement_id\"].isin([sentence_statement_id]))\n",
    "                & (prev_df[\"sentence_orig_idxs\"].isin([sentence_orig_idxs]))\n",
    "            ).any():\n",
    "                rows = prev_df[\n",
    "                    (prev_df[\"sentence_statement_id\"].isin([sentence_statement_id]))\n",
    "                    & (prev_df[\"sentence_orig_idxs\"].isin([sentence_orig_idxs]))\n",
    "                ]\n",
    "                if len(rows) != 1:\n",
    "                    print(rows)\n",
    "                    raise ValueError(f\"found multiple rows: {sentence_statement_id} : {sentence_orig_idxs}\")\n",
    "                # import pdb; pdb.set_trace()\n",
    "                row = rows.iloc[0]\n",
    "                data.append(\n",
    "                    [row.sentence_statement_id, row.sentence_orig_idxs, row.target_classes, row.predicted_classes]\n",
    "                )\n",
    "                already_there += 1\n",
    "            else:\n",
    "                target_classes = [int(x) for x in item[\"relevance\"][i, :]]\n",
    "                predicted_classes = [-1] * len(class_names)\n",
    "                if target_classes[class_of_interest_index] > -1:\n",
    "\n",
    "                    predicted_class, raw_result, processed_result = predict_fun(sentence_text, debug=debug)\n",
    "                    time.sleep(inter_query_wait_time_in_sec)\n",
    "\n",
    "                    predicted_classes[class_of_interest_index] = predicted_class\n",
    "                    count_sentence_was_annotated_with_class_of_interest += 1\n",
    "\n",
    "                    if predicted_class == -1:\n",
    "                        not_ok_results += 1\n",
    "                        not_ok_details.append([sentence_statement_id, sentence_orig_idxs, raw_result, processed_result])\n",
    "\n",
    "                else:\n",
    "                    count_sentence_was_not_annotated_with_class_of_interest += 1\n",
    "                data.append([sentence_statement_id, sentence_orig_idxs, target_classes, predicted_classes])\n",
    "                annotated += 1\n",
    "            # break\n",
    "            if max_amount > -1 and tot_count >= max_amount:\n",
    "                break\n",
    "            if print_count:\n",
    "                print(f\"done {tot_count} / {max_amount}\")\n",
    "        if max_amount > -1 and tot_count >= max_amount:\n",
    "            break\n",
    "            print(f\"reached the max amount of {max_amount}\")\n",
    "        # break\n",
    "finally:\n",
    "    df = pandas.DataFrame(\n",
    "        data, columns=[\"sentence_statement_id\", \"sentence_orig_idxs\", \"target_classes\", \"predicted_classes\"]\n",
    "    )\n",
    "    print(\n",
    "        f\"{already_there} sentences already in cache. {annotated} have been annotated.\\nOf these,\"\n",
    "        f\"{count_sentence_was_annotated_with_class_of_interest} had annotations for {class_of_interest}, and {count_sentence_was_not_annotated_with_class_of_interest} without.\"\n",
    "        f\"\\nTotal is {count_sentence_was_annotated_with_class_of_interest + count_sentence_was_not_annotated_with_class_of_interest}\"\n",
    "        f\"\\nNot ok results are {not_ok_results}\"\n",
    "    )\n",
    "    df.to_csv(f\"result_for_class_index_{class_of_interest_index}.csv\")\n",
    "\n",
    "    not_ok_df = pandas.DataFrame(\n",
    "        not_ok_details, columns=[\"statement_id\", \"sentence_orig_idxs\", \"raw_result\", \"processed_result\"]\n",
    "    )\n",
    "    not_ok_df.to_csv(f\"not_ok_for_class_index_{class_of_interest_index}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed23fb3-afab-411c-b58e-a28887140109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging\n",
    "# prev_df = pandas.read_csv(f\"result_for_class_index_{class_of_interest_index}.csv\")\n",
    "# prev_df[prev_df['sentence_statement_id'].isin([61]) & prev_df['sentence_orig_idxs'].isin([70])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d30204-ecc7-4a7e-99d4-f0fdb5ab0fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv(f\"result_for_class_index_{class_of_interest_index}.csv\")\n",
    "\n",
    "\n",
    "def parse_list(list_as_str):\n",
    "    stripped = list_as_str.strip(\"[]\")\n",
    "    lst = [int(x) for x in stripped.split(\",\")]\n",
    "    assert len(lst) == amount_of_classes\n",
    "    return lst\n",
    "\n",
    "\n",
    "preds = [parse_list(e)[class_of_interest_index] for e in list(df[\"predicted_classes\"])]\n",
    "targets = [parse_list(e)[class_of_interest_index] for e in list(df[\"target_classes\"])]\n",
    "stat_ids = df[\"sentence_statement_id\"]\n",
    "sent_ids = df[\"sentence_orig_idxs\"]\n",
    "assert len(preds) == len(targets) == len(stat_ids) == len(sent_ids)\n",
    "\n",
    "# note: preds are casted to 0 when they are -1, otherwise torchmertric will complain\n",
    "# this is ok because we still keep the target with the -1, so they will be ignored\n",
    "# Corrected version\n",
    "fixed_preds = [x if x > -1 else 0 for x in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec21348c-fc7c-4ed4-ae66-863c3112c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(preds)\n",
    "# print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893cd86f-fbf3-4323-87e8-50ec9ac4d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_fun = torchmetrics.classification.F1Score(task=\"binary\", ignore_index=-1)\n",
    "f1 = f1_fun(torch.tensor(fixed_preds), torch.tensor(targets))\n",
    "p_fun = torchmetrics.classification.Precision(task=\"binary\", ignore_index=-1)\n",
    "p = p_fun(torch.tensor(fixed_preds), torch.tensor(targets))\n",
    "r_fun = torchmetrics.classification.Recall(task=\"binary\", ignore_index=-1)\n",
    "r = r_fun(torch.tensor(fixed_preds), torch.tensor(targets))\n",
    "\n",
    "acc_fun = torchmetrics.classification.Accuracy(task=\"binary\", ignore_index=-1)\n",
    "acc = acc_fun(torch.tensor(fixed_preds), torch.tensor(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadd9484-5ac7-4ece-8fce-66ee1703d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"example amount is {len(preds)}\")\n",
    "print(f\"precision is {p:.3f}, recall is {r:.3f}, f1 is {f1:.3f}\")\n",
    "print(f\"accuracy is {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf7be29-a684-4703-a0a0-7105f82fc343",
   "metadata": {},
   "source": [
    "# Text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ee200a-d402-4090-b22b-dc80797976d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def index_data(dataloader):\n",
    "    data_as_dict = defaultdict(list)\n",
    "\n",
    "    for item in tqdm.tqdm(dataloader):\n",
    "        for i, sentence_text in enumerate(item[\"sentence_orig_text\"]):\n",
    "            sentence_statement_id = int(item[\"statement_id\"][i])\n",
    "            sentence_orig_idxs = item[\"sentence_orig_idxs\"][i]\n",
    "            assert len(sentence_orig_idxs) == 1\n",
    "            sentence_orig_idxs = int(sentence_orig_idxs[0])\n",
    "            data_as_dict[(sentence_statement_id, sentence_orig_idxs)].append(sentence_text)\n",
    "    return data_as_dict\n",
    "\n",
    "\n",
    "# train_dataloader_indexed = index_data(datamodule.train_dataloader())\n",
    "valid_dataloader_indexed = index_data(datamodule.val_dataloader())\n",
    "# test_dataloader_indexed = index_data(datamodule.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cf45d4-5935-4817-8b1a-d7ac83da40bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = [True] * len(preds)\n",
    "tp = [(p == 1) and (t == 1) for (p, t) in zip(preds, targets)]\n",
    "tn = [(p == 0) and (t == 0) for (p, t) in zip(preds, targets)]\n",
    "fp = [(p == 1) and (t == 0) for (p, t) in zip(preds, targets)]\n",
    "fn = [(p == 0) and (t == 1) for (p, t) in zip(preds, targets)]\n",
    "\n",
    "print(f\"all res is {len(all_res)} / tp is {sum(tp)} / tn is {sum(tn)} / fp is {sum(fp)}/ fn is {sum(fn)}\")\n",
    "\n",
    "\n",
    "def analyse_results(indexed_data, idxs_to_show, idx_from, idx_to):\n",
    "    count = 0\n",
    "    for i, should_show in enumerate(idxs_to_show):\n",
    "        if (i >= idx_from) and (i <= idx_to) and should_show:\n",
    "            count += 1\n",
    "            current = indexed_data[(stat_ids[i], sent_ids[i])]\n",
    "            print(f\"{current}\")\n",
    "    print(f\"displayed {count} examples\")\n",
    "\n",
    "\n",
    "analyse_results(valid_dataloader_indexed, fp, 0, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e51ea2e-4fce-4955-8c9a-8d71ee53e60a",
   "metadata": {},
   "source": [
    "# Duplicate check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9adae-8a69-46df-8dbf-29362880c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_duplicates(indexed_data):\n",
    "    count_dup = 0\n",
    "    count_tot = 0\n",
    "    for k, v in indexed_data.items():\n",
    "        if len(v) > 1:\n",
    "            assert all([v[0] == e for e in v])\n",
    "            # print(v)\n",
    "            count_dup += len(v)\n",
    "        count_tot += len(v)\n",
    "    print(f\"duplicates are there {count_dup} over {count_tot} ({count_dup / count_tot})\")\n",
    "\n",
    "\n",
    "# print(\"train\")\n",
    "# count_duplicates(train_dataloader_indexed)\n",
    "print(\"valid\")\n",
    "count_duplicates(valid_dataloader_indexed)\n",
    "# print(\"test\")\n",
    "# count_duplicates(test_dataloader_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceb013c-33ed-4a5c-a56c-c15e7e9ab4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "r = re.compile(\"no[^0-9a-zA-Z]\")\n",
    "string = \"no.\"\n",
    "if string == \"no\" or r.match(string):\n",
    "    print(\"ok\")\n",
    "else:\n",
    "    print(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa25c0f6-5e26-48c5-bbbe-e2c5073537ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
