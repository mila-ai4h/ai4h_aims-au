{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a525b35a-cf05-4e3c-ae2a-5e86dd3ec862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\",device='mps')\n",
    "\n",
    "hypothesis_ne = \"The statement '{}' downplays, denies, or avoids\"\n",
    "hypothesis_nne = \"The statement '{}' acknowledges\"\n",
    "\n",
    "# Labels\n",
    "targets = [\"Yes\", \"No\"]\n",
    "\n",
    "#EZ-STANCE: A Large Dataset for English Zero-Shot Stance Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df765bf-8dfe-476e-b33c-afc42ac91d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "df = pd.read_csv(\"./data/UK_context0_llama.csv\")\n",
    "ddf = df.dropna(subset=[\"comment\"])\n",
    "negative_instances = ddf[ddf.comment.apply(lambda x: \"negative\" in x)].index.tolist()\n",
    "idx = df.index.tolist()\n",
    "sentences = df.sentence.tolist()\n",
    "pred = df.predictions.tolist()\n",
    "pred = [ast.literal_eval(str(i)) if not isinstance(i, float) or not math.isnan(i) else [0 for ii in range(11)] for i in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5258e902-9029-41e9-8187-636d996d68db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_predictions(predictions, threshold=0.5):\n",
    "\n",
    "    return [1 if p > threshold else 0 for p in predictions]\n",
    "labels = []\n",
    "for p in pred:\n",
    "    labels.append(convert_predictions(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a08c223c-ef94-4368-8ec1-2349a5714965",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_id_list = df.iloc[:,0].tolist()\n",
    "statement_id_list = df.statement_id.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e04e720e-25a3-447d-a0c2-7cdd53c368c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "task_future_labels = []\n",
    "st = []\n",
    "for sentence, data, i in zip(sentences, np.array(labels), idx):\n",
    "    \n",
    "    for ii, lbs in enumerate(data[3:]):\n",
    "        if lbs == 1:\n",
    "            if i in negative_instances:\n",
    "                task_future_labels.append(1)\n",
    "            else:\n",
    "                task_future_labels.append(0)\n",
    "            st.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4b01b65-9c01-417d-b4d4-01f46be077ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006615598885793872"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negative_instances)/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c31aca50-f811-4b4a-a3eb-523ac758a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_name = [\n",
    "    \"approval\",\n",
    "    \"signature\",\n",
    "    \"c1 (reporting entity)\",\n",
    "    \"c2 (structure)\",\n",
    "    \"c2 (operations)\",\n",
    "    \"c2 (supply chains)\",\n",
    "    \"c3 (risk description)\",\n",
    "    \"c4 (risk mitigation)\",\n",
    "    \"c4 (remediation)\",\n",
    "    \"c5 (effectiveness)\",\n",
    "    \"c6 (consultation)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0038c3a-d0a7-4522-9674-3383d80b5277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b22f2802-c4e6-4a95-b0f6-7620d3adb24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "results = {key:[] for key in labels_name[3:]}\n",
    "predicted_results = []\n",
    "neg_scores = []\n",
    "pos_scores = []\n",
    "ct = {labels_name[idx+2]:0 for idx in range(9)}\n",
    "st_per_l = {labels_name[idx+2]:[] for idx in range(9)}\n",
    "score = []\n",
    "st_list = []\n",
    "new_prediction_list = []\n",
    "sen_id_list = []\n",
    "stat_id_list = []\n",
    "for sentence, data, i, si in zip(sentences, np.array(labels),sentence_id_list,statement_id_list):\n",
    "    \n",
    "    for idx, lbs in enumerate(data[3:]):\n",
    "        \n",
    "        if lbs == 1:\n",
    "            \n",
    "            result_ne = classifier(sentence, targets, hypothesis_template=hypothesis_ne)\n",
    "            result_nne = classifier(sentence, targets, hypothesis_template=hypothesis_nne)\n",
    "  \n",
    "            pos = result_ne['scores'][0] if result_ne['labels'][0] == targets[0] else result_ne['scores'][1]\n",
    "   \n",
    "            neg = result_nne['scores'][0] if result_nne['labels'][0] == targets[0] else result_nne['scores'][1]\n",
    "            \n",
    "            pos_scores.append(pos)\n",
    "            neg_scores.append(neg)\n",
    "            st_list.append(sentence)\n",
    "            new_prediction_list.append(data)\n",
    "            if pos - neg >0.15:\n",
    "                predicted_results.append(1)\n",
    "                ct[labels_name[idx+2]] += 1\n",
    "                st_per_l[labels_name[idx+2]].append(sentence)\n",
    "                score.append(pos/(pos+neg))\n",
    "            else:\n",
    "                predicted_results.append(0)\n",
    "                score.append(neg/(pos+neg))\n",
    "            sen_id_list.append(i)\n",
    "            stat_id_list.append(si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcfc7c69-1ef8-4dc5-8bad-c340961dc96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is  0.9663191659983962\n"
     ]
    }
   ],
   "source": [
    "correct = sum(t == p for t, p in zip(task_future_labels, predicted_results))\n",
    "print(\"accuracy is \", correct / len(task_future_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3238269-9668-4f54-9115-ffad3b8bc940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on not negative evident:  0.9700647249190939\n"
     ]
    }
   ],
   "source": [
    "pos = np.array(task_future_labels)==0\n",
    "correct = sum(t == p for t, p in zip(np.array(task_future_labels)[pos], np.array(predicted_results)[pos]))\n",
    "print(\"Accuracy on not negative evident: \", correct / len(np.array(task_future_labels)[pos]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60c7c071-7bca-474b-962a-223d0b6f80e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on negative evident:  0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "pos = np.array(task_future_labels)==1\n",
    "correct = sum(t == p for t, p in zip(np.array(task_future_labels)[pos], np.array(predicted_results)[pos]))\n",
    "print(\"Accuracy on negative evident: \", correct / len(np.array(task_future_labels)[pos]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be8cc555-c083-41ad-bb1b-cb16dc37e94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9663191659983962\n",
      "0.9760778144413623\n",
      "0.6025045537340619\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(task_future_labels, predicted_results, average='micro')) \n",
    "print(f1_score(task_future_labels, predicted_results, average='weighted'))\n",
    "print(f1_score(task_future_labels, predicted_results, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f3ff13f-373a-4c1d-8009-a6ee9e222fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"sentence_id\":sen_id_list, \"statement_id\":stat_id_list,\"sentence\":st_list, \"prediction\":predicted_results, \"targets\": task_future_labels,\"confidence\":score}).to_csv(\"./future_negative_evidence_results/negative_evidence_uk.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
