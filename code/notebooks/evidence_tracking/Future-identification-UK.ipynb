{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b6950c5-982e-4992-ae9e-9e074b10465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./data/UK_context0_llama.csv\")\n",
    "ddf = df.dropna(subset=[\"comment\"])\n",
    "future = ddf[ddf.comment.apply(lambda x: \"future\" in x)].index.tolist()\n",
    "idx = df.index.tolist()\n",
    "sentences = df.sentence.tolist()\n",
    "pred = df.predictions.tolist()\n",
    "pred = [ast.literal_eval(str(i)) if not isinstance(i, float) or not math.isnan(i) else [0 for ii in range(11)] for i in pred]\n",
    "\n",
    "\n",
    "def convert_predictions(predictions, threshold=0.5):\n",
    "\n",
    "    return [1 if p > threshold else 0 for p in predictions]\n",
    "labels = []\n",
    "for p in pred:\n",
    "    labels.append(convert_predictions(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f8214c-5eee-45dd-87c9-21a026a75984",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_id_list = df.iloc[:,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f357ed-4035-43c6-8a78-5009a2b8eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "statement_id_list = df.statement_id.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b59b457b-1ff9-4111-b594-7401d886c7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2872"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a92c8b1-f790-4603-99d8-c70e86c62d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_future_labels = []\n",
    "for sentence, data, i in zip(sentences, np.array(labels), idx):    \n",
    "    for ii, lbs in enumerate(data[3:]):\n",
    "        if lbs == 1:\n",
    "            if i in future:\n",
    "                task_future_labels.append(1)\n",
    "            else:\n",
    "                task_future_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2067e18d-c58b-4867-81b6-905faf8111b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(task_future_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a58e097-3c13-4ba7-bdb3-71842e8f7a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_name = [\n",
    "    \"approval\",\n",
    "    \"signature\",\n",
    "    \"c1 (reporting entity)\",\n",
    "    \"c2 (structure)\",\n",
    "    \"c2 (operations)\",\n",
    "    \"c2 (supply chains)\",\n",
    "    \"c3 (risk description)\",\n",
    "    \"c4 (risk mitigation)\",\n",
    "    \"c4 (remediation)\",\n",
    "    \"c5 (effectiveness)\",\n",
    "    \"c6 (consultation)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d1a0728-f43c-4027-8a37-676963b74b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/duoyizhang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/duoyizhang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure required NLTK models are downloaded\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "def identify_tense_nltk(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    tenses = {\"past\": 0, \"future\": 0}\n",
    "    is_future = False\n",
    "\n",
    "    # Expanded future-indicative words & phrases\n",
    "    # future_keywords = [\n",
    "    #     \"will\", \"intend\", \"expect\", \"aim\", \"plan\", \"anticipate\", \"future\",\n",
    "    #     \"going to\", \"shall\", \"may include\", \"plans to\", \"in 2024\", \"next year\",\n",
    "    #     \"at a later stage\", \"commit to\", \"schedule\", \"set to\", \"forecast\",\n",
    "    #     \"expected to\", \"proposed\", \"continue to\", \"implement\", \"develop\",\n",
    "    #     \"undertake\", \"consider\", \"establish\", \"introduce\", \"expand\",\n",
    "    #     \"launch\", \"explore\", \"assess\", \"enhance\", \"embed\", \"increase\",\n",
    "    #     \"next steps\", \"to be completed\"\n",
    "    # ]\n",
    "    future_keywords = [\n",
    "        \"will\", \"intend\", \"expect\", \"aim\", \"plan\", \"anticipate\", \"future\",\n",
    "        \"going to\", \"shall\", \"may include\", \"plans to\", \"2024\", \"next year\",\n",
    "        \"at a later stage\", \"next steps\", \"to be completed\", \"continue\",\"continuing\"\n",
    "    ]\n",
    "\n",
    "    # imperative_verbs = {\"insert\", \"implement\", \"embed\", \"incorporate\", \"continue\",\n",
    "    #                     \"recruit\", \"develop\", \"require\", \"ensure\", \"review\",\n",
    "    #                     \"provide\", \"publish\", \"categorize\", \"train\"}\n",
    "    imperative_verbs = {}\n",
    "\n",
    "    # Convert sentence to lowercase for easier keyword matching\n",
    "    sentence_lower = sentence.lower()\n",
    "\n",
    "    for i, (word, tag) in enumerate(tagged_words):\n",
    "        # # Count past tense verbs\n",
    "        # if tag == \"VBD\":\n",
    "        #     tenses[\"past\"] += 1\n",
    "        # Detect future modal verbs\n",
    "        if tag == \"MD\" and word.lower() in [\"will\", \"shall\"]:\n",
    "            tenses[\"future\"] += 1\n",
    "            is_future = True\n",
    "        # Identify infinitives that imply future actions (e.g., \"to implement\")\n",
    "        elif tag == \"TO\" and i + 1 < len(tagged_words) and tagged_words[i + 1][1] == \"VB\":\n",
    "            if tagged_words[i + 1][0].lower() in future_keywords:\n",
    "                tenses[\"future\"] += 1\n",
    "                is_future = True\n",
    "        if any(keyword in sentence_lower for keyword in future_keywords):\n",
    "            tenses[\"future\"] += 1\n",
    "            is_future = True\n",
    "\n",
    "    # Return \"future\" if future context was detected\n",
    "    if is_future:\n",
    "        return \"future\"\n",
    "\n",
    "    # Determine the dominant tense based on counts\n",
    "    #dominant_tense = max(tenses, key=tenses.get)\n",
    "    return \"no\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dd98993-04cb-4a24-bfc7-4c3bf25d973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "results = {key:[] for key in labels_name[3:]}\n",
    "predicted_results = []\n",
    "st = []\n",
    "new_list = []\n",
    "sen_id_list = []\n",
    "stat_id_list = []\n",
    "for sentence, data, i, si in zip(sentences, np.array(labels),sentence_id_list,statement_id_list):\n",
    "    for idx, lbs in enumerate(data[3:]):\n",
    "        if lbs == 1:\n",
    "            result = identify_tense_nltk(sentence)\n",
    "            if result == 'future':\n",
    "                predicted_results.append(1)\n",
    "            else:\n",
    "                predicted_results.append(0)\n",
    "            st.append(sentence)\n",
    "            new_list.append(data)\n",
    "            sen_id_list.append(i)\n",
    "            stat_id_list.append(si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0030938-58f3-424d-ac21-979a9fd9c61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1247"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bac997c3-70b1-4251-af95-3a77093e68d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"sentence_id\":sen_id_list, \"statement_id\":stat_id_list,\"sentence\":new_list, \"prediction\":predicted_results, \"targets\": task_future_labels}).to_csv(\"./future_negative_evidence_results/future_uk.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19ed0269-dd8c-47ef-ac8d-130f65dd10e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy is  0.863672814755413\n"
     ]
    }
   ],
   "source": [
    "correct = sum(t == p for t, p in zip(task_future_labels, predicted_results))\n",
    "print(\"Overall accuracy is \", correct / len(task_future_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce158706-f395-4cbf-a301-515c07684e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on prediction future works:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "pos = np.array(task_future_labels)==1\n",
    "correct = sum(t == p for t, p in zip(np.array(task_future_labels)[pos], np.array(predicted_results)[pos]))\n",
    "print(\"Accuracy on prediction future works: \", correct / len(np.array(task_future_labels)[pos]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d630536e-620b-4342-a98d-66d5d5eb8258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.863672814755413\n",
      "0.8861259763905242\n",
      "0.6683303085299456\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(task_future_labels, predicted_results, average='micro')) \n",
    "print(f1_score(task_future_labels, predicted_results, average='weighted'))\n",
    "print(f1_score(task_future_labels, predicted_results, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
